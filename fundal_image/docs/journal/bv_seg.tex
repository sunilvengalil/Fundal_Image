% TODO add sample annotated images for OD, also show the gt for all
% TODO  List Survey on architecture search
% TODO Report the result for all, mention which is the best and also mention whether it can be improved or not.
% TODO Related work - Exudates
% TODO addd more images in results and discuss - make subsections    (Result and Discussion make separate section)
% TODO Expeirment performed - multiple section
% TODO comparison with existing work



% Template for ISBI paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------


\documentclass{article}
\usepackage{spconf, amsmath, graphicx}
\usepackage{url}
% It's fine to compress itemized lists if you used them in the
% manuscript
\usepackage{enumitem}
\setlist{nosep, leftmargin=14pt}

\usepackage{mwe} % to get dummy images
% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{Simultaneous Segmentation of Multiple Structures in Fundal Images using Multi-tasking Deep Neural Networks}
%
% Single address.
% ---------------
%\name{Sunil Kumar Vengalil}
%\address{International Institute of Information Technology}
%
% For example:
% ------------
%\address{School\\
%  Department\\
%  Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Some author footnote.}}
%  {School A-B\\
%  Department A-B\\
%  Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%  while at ...}}
%  {School C-D\\
%  Department C-D\\
%  Address C-D}
%
% More than two addresses
% -----------------------
% \name{Sunil Kumar Vengalil^{\star \dagger}$ \qquad Bharath K^{\star}$ \qquad Neelam Sinha^{\dagger}$}
%
% \address{$^{\star}$ International Institute of Information Technology \\
%     $^{\dagger}$}Affiliation Number Two
%
\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
Fundal imaging is the most commonly used non-invasive technique for early detection of many retinal diseases like diabetic retinopathy. An initial step in automatic processing of fundal images for detecting diseases is to identify the various landmark regions like optic disc, blood vessels and fovea. In addition to these, various abnormalities like exudates that help in pathological analysis are also visible in fundal images. In this work, we propose a multi-tasking deep learning architecture for segmenting optic disc, blood vessels, fovea and exudates simultaneously. Our experimental results on publicly available datasets show that simultaneous segmentation of all these structures results in significant improvement in the performance. For segmentation performance on blood vessels we got Dice score of 80.31\%, 78.11\% and \% on the datasets DRIVE, HRF, and CHASE\_DB respectively. On Exudates, we got a Dice score of 65\% on IDRiD dataset when trained in combination with Optic Disc (OD) and Blood Vessels using multi-tasking loss function, whereas the Dice score when trained individually is 50\%. To the best of our knowledge, we  are the first one to evaluate the effectiveness of  multi-task learning for segmenting multiple structures in fundal images. We obtained a state of the art Accuracy score of 95.89\% for blood vessel segmentation on 20 DRIVE test images which is 1.39\% higher than one of the recently reported studies.
\end{abstract}
%
\begin{keywords}
Fundal Image Segmentation, Multi-task learning, Deep Learning
\end{keywords}
%

\section{Introduction}
Fundal imaging, capturing images of retina using specialized cameras, is the most widely used non-invasive technique for screening of retinal diseases.
These images are used to identify common eye diseases like diabetic retinopathy, which is the most common cause for blindness, and many other cardiovascular diseases. Blood vessels, Optic Disc (OD) and Macula are the major structures visible in a fundal image. However, manual identification and demarcation of fine structures like blood vessels take a lot of time and effort. Hence automatic detection of major landmarks in fundal image has become an active research area.

Figure \ref{sample_funal_image} shows a fundal image with various structures like blood vessels, optic disc and fovea marked. The optic disc is the point of exit of the optic nerves that carry information from the eye to the brain. It is also the point where all the blood vessels enter the eye.Since there are no photo sensors (rods and cones) present in the optic disc, it corresponds to a blind spot in the retina.Fovea is a small region with a lot of cone cells packed together and hence this region is responsible for sharp vision. Blood vessels that carry blood to the eye are spread across the entire region of the retina and vary in thickness and density.

Figure \ref{fundal_image_exudates} shows a fundal image with Exudates and corresponding ground truth. Exudates are the fluid (pus) leaking out of blood vessels. It is found mostly in Diabetic Retinopathy patients. Exudates appear as white patches in the fundal image.

Since the break-through success of deep learning in solving tasks in domains like computer vision for classification \cite{krizhevsky2012imagenet} \cite{simonyan2014very} and segmentation \cite{chen2017deeplab}, many deep learning architectures have been tried for segmenting important structures, such as optic disc and blood vessels, in fundal images \cite {vengalil2016customizing} \cite{zhuang2018laddernet} \cite{jiang2018retinal} \cite{park2020m}. One of the challenges of using deep learning architecture for medical images is the lack of annotated training data. Many approaches, like taking multiple training patches from  a single image \cite{vengalil2016customizing} and transfer learning, where a model trained on a dataset such as the Imagenet \cite{deng2009imagenet} is fine tuned for the task at hand, are proposed and found to be successful.

In this work, we propose a multi-tasking deep learning architecture for simultaneous detection of blood vessels, optic disc, fovea and exudates. Our results show that a single network that predicts multiple structures  performs much better compared to detecting  each structure independently using different networks as the single network can make use of the correlation between the two tasks. This correlation is evident from Figure \ref{sample_funal_image}, where one can see that near and inside the optic disc blood vessels are thicker and denser. So the knowledge of the optic disc can help the prediction task for blood vessels and vice versa. We perform experiments and report results on three popular datasets DRIVE \cite{drive}, HRF \cite{budai2013robust} and IDRID \cite{h25w98-18}.

The contributions of our work are:
\begin{enumerate}
\item Propose a multi-tasking model for simultaneous segmentation of blood vessels, optic discs, fovea and exudates.
\item Propose a method for data augmentation of fundal images which enables the training and prediction on whole images, rather than using image patches.
Using the entire image provides more contextual information that can help the segmentation task.
\item We report a state of the art accuracy of 79.76\% on DRIVE test images which is 3.76\% higher than the results reported by Adapa et. al. \cite{adapa2020supervised}.
\item We could improve the F1- score on DRIVE test dataset to  80.31\% by optimizing the dimension of latent representation and number of channels in the bottleneck layer of U-Net architecture.

\end{enumerate}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.5\linewidth]{images/eye_1.png}
\caption{Sample fundal image showing important structures Blood Vessels, Optic Disc and Macula}
\label{sample_funal_image}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.5\linewidth]{images/Exudates_Image.png}
\caption{Sample fundal image with Exudates}
\label{fundal_image_exudates}
\end{figure}

\section{Related Work}
Existing  techniques for fundal image segmentation mainly fall under two categories 1) using traditional image processing techniques and 2) using deep learning techniques.Examples of image processing based techniques include filter based \cite{zhang2010retinal} \cite{yavuz2011retinal} \cite{aslan2018segmentation} and morphological operations based \cite{hassan2015retinal}\cite{singh2014new}.
Image processing based techniques have the advantage that they donâ€™t need the ground truth images, but their performance is far behind the deep learning based approaches.Further, these algorithms are based on many tunable thresholds that vary from dataset to dataset and they work based on some assumptions like gaussian distribution.

Several deep learning architectures that were successful in segmentation tasks \cite{chen2017deeplab} in natural images were tried for segmenting blood vessels in retinal images and the results were significantly better than using conventional image processing techniques.
In \cite{vengalil2016customizing} a state-of-the-art segmentation model deeplab \cite{chen2017deeplab}, which is pre-trained on natural images  for semantic segmentation,  was used to segment blood vessels at pixel level.Jiang et.al. proposes \cite {jiang2018retinal} a  pre-trained fully convolutional network for segmenting blood vessels and report accuracy of cross-dataset test on four different datasets.In M-GAN \cite{park2020m}, introduced by Kyeong et.al., a multi-kernel pooling block added between stacked convolutional layers supports scale-invariance which is a highly desirable feature for blood vessel segmentation.

One of the main challenges in using deep neural networks for segmentation tasks is that the reduction in resolution of featuremap as one goes deeper will result in loss of finer details like edges, which are crucial for segmentation tasks.In order to circumvent this, the U-Net \cite{ronneberger2015u} model was introduced specifically for medical image segmentation which has multiple skip connections.
In their recent work, Joshua et.al. \cite{joshua2020blood} used a modified version of U-NET for segmenting blood vessels in retinal images and reported state of the art accuracies.
In Laddernet, introduced by Zhuang et.al. in 2018 \cite{zhuang2018laddernet},  is a sequence of multiple U-Nets cascaded together.

\section{Proposed Method}
\subsection{CNN architecture}
A modified version of the UNET architecture \cite{ronneberger2015u}, shown in Fig \ref{unet_combined}, is used for segmenting various structures. The modification are:
\begin{enumerate}
\item The proposed network retains the original dimensions of the input image, whereas the original U-Net mentioned in \cite{ronneberger2015u} reduces the original  input size from $572 \times 572$ to $388 \times 388$.

\item We used de-convolutional layers with stride for upsampling as opposed to  upsampling layers used in original U-Net architecture. Our method is preferred as we learned the interpolation weights using a deconvolutional layer.
\item We added batch normalization after each convolutional layer in order to stabilize the training process as well as for faster training.
\end{enumerate}

The encoder and decoder consists of 4 stages each.
Each stage of the encoder comprises two convolution layers, each followed by batch normalization and ReLU activation function.
A max-pooling layer with a stride of 2 was added at the end of each stage of the encoder which down samples the image by a factor of two.
Each decoder layer up-samples the image by a factor of 2 using  a deconvolution layer followed by a convolution layer. The sigmoid function is used in the final output channel with two filters instead of the softmax activation function because the blood vessel and optic disc features are  mutually inclusive. These features share common connections in the fundal image, and hence, their simultaneous segmentation also yields the best results.

For experiments on individual prediction the network had one output channel corresponding to the segmentation map.
The network outputs a binary image, of the same resolution as the input,  which indicates pixel by pixel segmentation.
For multi-tasking models, an additional channel for predicting the optic disc in combination with another structure  is added at the output layer.
We chose to combine optic disc with other structures as optic disc annotation was available for most of the datasets.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.9\linewidth]{images/UnetArch.pdf}
\caption{Architecture of the proposed multi-tasking U-Net model}
\label{unet_combined}
\end{figure}

We also evaluate models with different dimensions of latent representation and different number of channels  in the bottleneck layer and report the results for all combinations. For latent representation dimension, we experimented with different values 16, 32, 64 and 128. For the number of channels, experiments are carried out with 384, 512, 768, 1024 and 1280.

\subsection{Dataset}
We used the DRIVE \cite{drive}, HRF \cite{budai2013robust}, CHASE\_DB\cite{chase_db} and IDRiD\cite{idrid},  datasets. The DRIVE dataset contains 20 training images and 20 testing images of resolution $565 \times 584$ pixels. The dataset also provides ground truth images for blood vessel segmentation annotated by a human expert. As the DRIVE and CHASE\_DB dataset does not have optic disc annotations, we annotated the optic disc in each image ourselves. HRF dataset has 15 high resolution fundal images along with ground truth annotation for blood vessel segmentation.

IDRID \cite{h25w98-18} localization dataset, which contains  413 training images and 103 test images along with fovea ground truth, was used for fovea localization. For exudates segmentation, we used IDRID \cite{h25w98-18} segmentation dataset which contains a total of 81 images.

Data-augmentation - horizontal and vertical flipping, grid and elastic distortion provided by the library Albumentations \cite{buslaev2020albumentations}, was used  to increase the number of training samples by a factor of 4.

\subsection{Experiments performed}
We used full images, as opposed to image patches, for training the network  as a full image will have more context and hence can be more effective for predicting structures like optic disc and fovea. We performed multiple experiments, for individual and simultaneous prediction of blood vessels, optic disc, fovea and exudates.

Firstly, we performed blood vessel segmentations on the individual datasets with the proposed U-Net architecture to achieve the best possible results. To further improve the results obtained on the blood vessels, we utilized simultaneous segmentation of blood vessels and optic disc. Apart from these experiments, we also modified the U-Net architecture to find the best suitable parameters for both blood vessels and optic disc.

Another experiment we carried out is to train a model for blood vessel segmentation using training set containing images from HRF, DRIVE and CHASE datasets and we evaluated the performance of this trained model on  1) on a held out validation dataset which comprises images from HRF, DRIVE and CHASE  2) test dataset containing images from IDRiD dataset. On the IDRiD dataset, we evaluate the model qualitatively as no ground truth was available for this dataset. Further in order to measure the generalizability of the trained model we also report across the dataset results for all tasks.

The obtained blood vessel results on the IDRiD dataset is further utilized to improve the optic disc, fovea and exudates segmentation results.



\subsection{Loss function and training}\label{sec:loss_and_training}
A sigmoid activation function is used at the output layer. The model is trained with dice loss and  with a combination of dice loss and binary cross entropy loss. In most of the experiments, we noticed that the combination of two losses gave a better F1-score.

For predicting the structures separately, four separate networks with the same architecture were trained independently one for each of the structures: blood vessel, optic disc, fovea and exudates.

In the multi-tasking model, we trained three separate networks each with two output channels for predicting the following three combinations:
\begin{enumerate}
\item Blood vessel and  optic disc
\item Fovea and optic disc
\item Exudates and optic disc
\end{enumerate}

The network was trained for 60 epochs in all the cases.

\begin{figure}[!ht]
\begin{minipage}[b]{0.7\linewidth}
\centering
\centerline{\includegraphics[width=6cm]{images/19_test_img.png}}
\centerline{(a) Blood Vessels}\medskip
\end{minipage}

\begin{minipage}[b]{0.7\linewidth}
\centering
\centerline{\includegraphics[width=6cm]{images/19_test_gt.png}}
\centerline{(b) Optic Disc}\medskip
\end{minipage}

\begin{minipage}[b]{0.7\linewidth}
\centering
\centerline{\includegraphics[width=6cm]{images/19_test prediction.png}}
\centerline{(c) Exudates}\medskip
\end{minipage}

\caption{Sample Blood Vessel segmentation results for DRIVE, HRF and CHASE\_DB datasets with Multi-tasking. The green pixels in the predicted image correspond to true positives, blue pixels correspond to false positives and  red pixels correspond to false negatives. F1-score is 86.01\% with a precision of 84.63\% and recall of 87.44\% }
\label{fig:segmentation_results_with_multi_tasking}

\end{figure}

\begin{figure}[!ht]
\begin{minipage}[b]{0.7\linewidth}
\centering
\centerline{\includegraphics[width=6cm]{images/19_test_img.png}}
\centerline{(a) Blood Vessels}\medskip
\end{minipage}

\begin{minipage}[b]{0.7\linewidth}
\centering
\centerline{\includegraphics[width=6cm]{images/19_test_gt.png}}
\centerline{(b) Optic Disc}\medskip
\end{minipage}

\begin{minipage}[b]{0.7\linewidth}
\centering
\centerline{\includegraphics[width=6cm]{images/19_test_pred_individual.png}}
\centerline{(c) Exudates}\medskip
\end{minipage}

\caption{Sample Blood Vessel segmentation results for DRIVE, HRF and CHASE\_DB datasets without Multi-tasking. The green pixels in the predicted image correspond to true positives, blue pixels correspond to false positives and  red pixels correspond to false negatives. The F-score is 70.83\% with a precision of 56.40\% and recall of 91.12\%}
\label{fig:segmentation_results}

\end{figure}

\section{Results and Discussion}\label{sec:results}

%\begin{table*}[htbp]
%\caption{Comparison of results with and without multi-tasking. For all the tasks except OD, multi-tasking was done with OD as auxiliary task. For OD, multi-tasking was done in combination with blood vessels.}
%\begin{center}
%\begin{tabular}{|c|c|c|c|c|c|}
%\hline
%&&\multicolumn{2}{|c|}{\textbf{Individual}}& \multicolumn{2}{|c|}{\textbf{Multi-tasking}} \\
%\cline{3-6}
%\textbf{Task} &\textbf{\textit{Dataset}}& \textbf{\textit{Dice(\%)}}& \textbf{\textit{IoU(\%)}}& \textbf{\textit{Dice(\%)}}& \textbf{\textit{IoU(\%)}}  \\
%\hline
%Blood Vessel & DRIVE & 76.35 & 61.96 & 79.76 & 66.5  \\
%\cline{2-6}
%& HRF & 76.11 & 61.62 &  82.24 & 69.86 \\
%\cline{2-6}
%& CHASE\_DB & 74.62 & 59.59 & 78.27 & 64.35 \\
%\hline
%Optic Disc& DRIVE& 76.24 & 66.15 & 78.63 & 69.85  \\
%\cline{2-6}
%&IDRID& 88(\%) & 81 & 94 & 90 \\
%\hline
%Fovea& IDRID & 70 & 61 &66 &57  \\
%\hline
%Exudates&IDRID& 50 & 34 & 60 & 45  \\
%\hline
%\end{tabular}
%\label{tab:results}
%\end{center}
%\end{table*}

\begin{table*}[htbp]
\caption{Comparison of results with and without multi-tasking. For all the tasks except OD, multi-tasking was done with OD as auxiliary task. For OD, multi-tasking was done in combination with blood vessels.}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
&&\multicolumn{2}{|c|}{\textbf{Individual}}& \multicolumn{2}{|c|}{\textbf{Multi-tasking}} \\
\cline{3-6}
&\textbf{\textit{Dataset}}& \textbf{\textit{Dice(\%)}}& \textbf{\textit{IoU(\%)}}& \textbf{\textit{Dice(\%)}}& \textbf{\textit{IoU(\%)}}  \\
\hline
Blood Vessel & DRIVE & 77 & 62.78 & 80.31 & 67.35  \\
\cline{2-6}
& HRF & 78.11 & 64.29 &  & \\
\cline{2-6}
& CHASE\_DB &  &  &  & \\
\hline
Optic Disc & DRIVE & 76.24 & 66.15 & 78.63 & 69.85  \\
\cline{2-6}
&IDRID & 88(\%) & 81 & 94 & 90 \\
\hline
Fovea& IDRID & 70 & 61 &66 &57  \\
\hline
Exudates & IDRID & 50 & 34 & 60 & 45  \\
\hline
\end{tabular}
\label{tab:results}
\end{center}
\end{table*}

Table \ref{tab:results} compares segmentation performance of various structures when the model is trained with and without multi-tasking. The table provides a comparison of  results when  the model is trained in multi-tasking mode with two different tasks  Vs the results with individual training. In all these cases, Optic Disc is added as an auxiliary task along with other main tasks like Blood Vessels, Macula and Exudates. The row for optic disc shows comparison of segmentation performance of optic disc when trained with optic disc alone Vs results of multi-tasking loss with blood vessels.  As evident from the table, Dice score for exudates segmentation resulted in an improvement  of 10\% when the model was trained in combination with optic disc. A similar trend is observed in blood vessel and optic disc segmentation with an improved Dice score of 6\% (for HRF dataset) and 6\% (for IDRiD dataset)  for Optic Disc and Blood Vessels respectively. For Macula, the segmentation results decreased by 2\% upon addition of Optic Disc as an additional task during training. This happens because the fovea and optic disc are mutually exclusive as they appear at different locations in the image.

However, when trained with blood vessels, Macula and OD together we observed a 3\% increase in the score for Macula. Similarly, when multi-tasking was done with three tasks (Optic Disc, Exudates and Blood Vessels segmentation) the score for Exudates segmentation also increased by 15\% to a value of 65\%.


Figure \ref{sample_blood vessels} shows sample results for blood vessel segmentation task for HRF, DRIVE and CHASE\_DB  when the model is trained onlly for that task and Figure \ref{sample_blood_vessels_multi_tasing} shows the results with multi-tasking.

The increase in Dice score is due to less number of false positives in the prediction( resulting in larger precision).

\begin{table}
\caption{Results of various multi-tasking experiments run on segmentation of Exudates. We got the best results when the model is trained with multi-tasking loss for a combination of Exudates, Optic Dic and Blood Vessels.}
\begin{center}
\begin{tabular}{ |l|c|  }
\hline
\textbf{Experiments Run} & \textbf{Dice(\%)} \\
\hline
Exudates Alone & 50 \\
\hline
Exudates and OD & 53 \\
\hline
Exudates and Blood Vessels & 61 \\
\hline
\textbf{Exudates, OD and Blood Vessels} & \textbf{65} \\
\hline
\end{tabular}
\end{center}
\end{table}

The improved performance with multi-tasking is a consequence of direct correlation between the two predicted structures. When trained together, the network is able to learn new hidden layer features that can contribute to the prediction of both structures. When trained individually, the optic disc can easily be confused as exudates. In simultaneous segmentation, the network learns to discriminate optic disc and exudates which improves the segmentation results for both.

Figure \ref{fig:segmentation_results} shows sample segmentation output for  blood vessels, optic disc, fovea and exudates with multi-tasking loss function.

Figure \ref{fig:z_dim_vs_dice} shows the plot of validation Dice score Vs latent representation  dimension and Fig \ref{}  shows Dice score as a function of  number of channels in the bottleneck layer. As evident from the figure....

Figure shows the results of blood vessel segementtion on IDRiD dataset using the model trained using images from  DRIVE, HRF and CHASE\_DB datasets.


\section{Conclusion}
In this work, we illustrate the effectiveness of multi-tasking deep learning models for segmenting blood vessels, optic disc, fovea and exudates in fundal images. We build a multi-tasking model, and corresponding loss function,  based on the famous U-Net architecture for simultaneous  segmentation of multiple structures. Using the proposed approach we report a Dice score of 78.5\%, 94\%, 66\% and 60\% on segmentation of blood vessels, optic disc, fovea and exudates respectively.
The proposed approach resulted in  a peak increase of 10\% in Dice score for exudates segmentation compared to the individual segmentation result with the same architecture. Using the proposed approach we were able to get a state of the art accuracy of 78\% on DRIVE test images which is 2\% greater than one of the recently reported results.

\section{Compliance with ethical standards}
\label{sec:ethics}
No ethical approval was required for this study as we used only publicly available datasets.
%% References should be produced using the bibtex program from suitable
%% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
%% style file from IEEE produces unsorted bibliography list.
%% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{strings,refs}

\end{document}
