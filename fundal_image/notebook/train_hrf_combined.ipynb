{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_hrf_combined.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mdyi7smT1LaS"
      },
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Recall, Precision\n",
        "# from model import build_unet\n",
        "# from metrics import dice_loss, dice_coef, iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf0LEN871QsT"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def iou(y_true, y_pred):\n",
        "    def f(y_true, y_pred):\n",
        "        intersection = (y_true * y_pred).sum()\n",
        "        union = y_true.sum() + y_pred.sum() - intersection\n",
        "        x = (intersection + 1e-15) / (union + 1e-15)\n",
        "        x = x.astype(np.float32)\n",
        "        return x\n",
        "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n",
        "\n",
        "smooth = 1e-15\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0Cbk_hC1RmB"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def conv_block(inputs, num_filters):\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder_block(inputs, num_filters):\n",
        "    x = conv_block(inputs, num_filters)\n",
        "    p = MaxPool2D((2, 2))(x)\n",
        "    return x, p\n",
        "\n",
        "def decoder_block(inputs, skip_features, num_filters):\n",
        "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n",
        "    x = Concatenate()([x, skip_features])\n",
        "    x = conv_block(x, num_filters)\n",
        "    return x\n",
        "\n",
        "def build_unet(input_shape):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    s1, p1 = encoder_block(inputs, 64)\n",
        "    s2, p2 = encoder_block(p1, 128)\n",
        "    s3, p3 = encoder_block(p2, 256)\n",
        "    s4, p4 = encoder_block(p3, 512)\n",
        "\n",
        "    b1 = conv_block(p4, 1024)\n",
        "\n",
        "    d1 = decoder_block(b1, s4, 512)\n",
        "    d2 = decoder_block(d1, s3, 256)\n",
        "    d3 = decoder_block(d2, s2, 128)\n",
        "    d4 = decoder_block(d3, s1, 64)\n",
        "\n",
        "    outputs = Conv2D(2, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
        "\n",
        "    model = Model(inputs, outputs, name=\"UNET\")\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYDrko4g1TpY"
      },
      "source": [
        "H = 512\n",
        "W = 768\n",
        "\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def load_data(path):\n",
        "    x = sorted(glob(os.path.join(path, \"image\", \"*.jpg\")))\n",
        "    y = sorted(glob(os.path.join(path, \"mask\", \"*.jpg\")))\n",
        "    return x, y\n",
        "\n",
        "def shuffling(x, y):\n",
        "    x, y = shuffle(x, y, random_state=42)\n",
        "    return x, y\n",
        "\n",
        "def read_image(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    # x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "def read_mask(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)  \n",
        "    # x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    x = x[:,:,0:2] # Discard the last channel \n",
        "\n",
        "    return x\n",
        "\n",
        "def tf_parse(x, y):\n",
        "    def _parse(x, y):\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y)\n",
        "        return x, y\n",
        "\n",
        "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
        "    x.set_shape([H, W, 3])\n",
        "    y.set_shape([H, W, 2])\n",
        "    return x, y\n",
        "\n",
        "def tf_dataset(X, Y, batch_size=2):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "    dataset = dataset.map(tf_parse)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(4)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUNXf67ESWvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0519d35-f671-4bee-eb4f-c8622ab361a3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQOrZxMwSs4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a7fc939-33d4-43ef-b9d2-a1112f1e80a8"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/sunil/icvgip/fundal_image/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/sunil/icvgip/fundal_image\n",
            "drive.zip      files_od_and_bv_weighted_demo_1\t  HRF.zip    trained_models\n",
            "files\t       fundal_image-1.0-py3-none-any.whl  logs\n",
            "files_demo_bv  HRF\t\t\t\t  new_data\n",
            "files_demo_od  hrf_resized\t\t\t  new_data4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX_cRlg112I6"
      },
      "source": [
        "#!unzip new_data2.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7pR8K3ul8E4"
      },
      "source": [
        "model_dir = \"trained_models\"\n",
        "model_name = \"model_hrf_combined\"\n",
        "result_dir = \"hrf_combined_results\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvjtnxZS12zR",
        "outputId": "75463883-3b90-484b-e60a-e1b8bc8182b5"
      },
      "source": [
        "\"\"\" Seeding \"\"\"\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "\"\"\" Directory to save files \"\"\"\n",
        "create_dir(model_dir)\n",
        "create_dir(result_dir)\n",
        "\n",
        "\"\"\" Hyperparameters \"\"\"\n",
        "batch_size = 2\n",
        "lr = 1e-4\n",
        "num_epochs = 30\n",
        "model_path = os.path.join(model_dir, f\"{model_name}.h5\")\n",
        "csv_path = os.path.join(result_dir, \"training_history.csv\")\n",
        "\n",
        "\"\"\" Dataset \"\"\"\n",
        "dataset_path = \"hrf_resized\"\n",
        "train_path = os.path.join(dataset_path, \"train\")\n",
        "valid_path = os.path.join(dataset_path, \"test\")\n",
        "\n",
        "train_x, train_y = load_data(train_path)\n",
        "train_x, train_y = shuffling(train_x, train_y)\n",
        "valid_x, valid_y = load_data(valid_path)\n",
        "\n",
        "print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
        "print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n",
        "\n",
        "train_dataset = tf_dataset(train_x, train_y, batch_size=batch_size)\n",
        "valid_dataset = tf_dataset(valid_x, valid_y, batch_size=batch_size)\n",
        "\n",
        "train_steps = len(train_x)//batch_size\n",
        "valid_setps = len(valid_x)//batch_size\n",
        "\n",
        "if len(train_x) % batch_size != 0:\n",
        "    train_steps += 1\n",
        "if len(valid_x) % batch_size != 0:\n",
        "    valid_setps += 1\n",
        "\n",
        "\"\"\" Model \"\"\"\n",
        "model = build_unet((H, W, 3))\n",
        "# model.compile(loss=dice_loss, optimizer=Adam(lr), metrics=[dice_coef, iou, Recall(), Precision()])\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=Adam(lr), metrics=[dice_coef, iou, Recall(), Precision()])\n",
        "# model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, min_lr=1e-6, verbose=1),\n",
        "    CSVLogger(csv_path),\n",
        "    TensorBoard(),\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=False)\n",
        "]\n",
        "\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=valid_dataset,\n",
        "    steps_per_epoch=train_steps,\n",
        "    validation_steps=valid_setps,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 60 - 60\n",
            "Valid: 5 - 5\n",
            "Epoch 1/30\n",
            "30/30 [==============================] - 72s 328ms/step - loss: 0.4816 - dice_coef: 0.1595 - iou: 0.0868 - recall_2: 0.2316 - precision_2: 0.5461 - val_loss: 0.6610 - val_dice_coef: 0.1047 - val_iou: 0.0553 - val_recall_2: 0.0012 - val_precision_2: 0.2330\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.66102, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 2/30\n",
            "30/30 [==============================] - 7s 240ms/step - loss: 0.3151 - dice_coef: 0.1892 - iou: 0.1046 - recall_2: 0.0817 - precision_2: 0.9335 - val_loss: 0.7165 - val_dice_coef: 0.0983 - val_iou: 0.0517 - val_recall_2: 0.3932 - val_precision_2: 0.2826\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.66102\n",
            "Epoch 3/30\n",
            "30/30 [==============================] - 7s 241ms/step - loss: 0.2712 - dice_coef: 0.2272 - iou: 0.1283 - recall_2: 0.1015 - precision_2: 0.9725 - val_loss: 0.4878 - val_dice_coef: 0.0986 - val_iou: 0.0519 - val_recall_2: 1.4497e-05 - val_precision_2: 0.0265\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.66102 to 0.48784, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 4/30\n",
            "30/30 [==============================] - 7s 242ms/step - loss: 0.2473 - dice_coef: 0.2558 - iou: 0.1468 - recall_2: 0.1155 - precision_2: 0.9776 - val_loss: 0.4068 - val_dice_coef: 0.0955 - val_iou: 0.0502 - val_recall_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.48784 to 0.40684, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 5/30\n",
            "30/30 [==============================] - 7s 241ms/step - loss: 0.2303 - dice_coef: 0.2796 - iou: 0.1627 - recall_2: 0.1257 - precision_2: 0.9794 - val_loss: 0.3374 - val_dice_coef: 0.0933 - val_iou: 0.0489 - val_recall_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.40684 to 0.33742, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 6/30\n",
            "30/30 [==============================] - 7s 240ms/step - loss: 0.2186 - dice_coef: 0.2954 - iou: 0.1735 - recall_2: 0.1284 - precision_2: 0.9817 - val_loss: 0.2935 - val_dice_coef: 0.0919 - val_iou: 0.0482 - val_recall_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.33742 to 0.29353, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 7/30\n",
            "30/30 [==============================] - 7s 240ms/step - loss: 0.2115 - dice_coef: 0.3078 - iou: 0.1821 - recall_2: 0.1306 - precision_2: 0.9812 - val_loss: 0.4153 - val_dice_coef: 0.0728 - val_iou: 0.0378 - val_recall_2: 0.0510 - val_precision_2: 0.2613\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.29353\n",
            "Epoch 8/30\n",
            "30/30 [==============================] - 7s 241ms/step - loss: 0.2046 - dice_coef: 0.3129 - iou: 0.1857 - recall_2: 0.1275 - precision_2: 0.9792 - val_loss: 0.2571 - val_dice_coef: 0.0797 - val_iou: 0.0415 - val_recall_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.29353 to 0.25714, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 9/30\n",
            "30/30 [==============================] - 7s 240ms/step - loss: 0.1947 - dice_coef: 0.3274 - iou: 0.1960 - recall_2: 0.1305 - precision_2: 0.9829 - val_loss: 0.2356 - val_dice_coef: 0.0891 - val_iou: 0.0467 - val_recall_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.25714 to 0.23557, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 10/30\n",
            "30/30 [==============================] - 7s 241ms/step - loss: 0.1879 - dice_coef: 0.3380 - iou: 0.2036 - recall_2: 0.1312 - precision_2: 0.9853 - val_loss: 0.2286 - val_dice_coef: 0.0938 - val_iou: 0.0492 - val_recall_2: 5.4173e-05 - val_precision_2: 0.4383\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.23557 to 0.22862, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 11/30\n",
            "30/30 [==============================] - 7s 241ms/step - loss: 0.1818 - dice_coef: 0.3479 - iou: 0.2109 - recall_2: 0.1319 - precision_2: 0.9870 - val_loss: 0.2187 - val_dice_coef: 0.1216 - val_iou: 0.0648 - val_recall_2: 0.0120 - val_precision_2: 0.8025\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.22862 to 0.21872, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 12/30\n",
            "30/30 [==============================] - 7s 241ms/step - loss: 0.1762 - dice_coef: 0.3575 - iou: 0.2180 - recall_2: 0.1328 - precision_2: 0.9879 - val_loss: 0.2126 - val_dice_coef: 0.1002 - val_iou: 0.0528 - val_recall_2: 2.9223e-04 - val_precision_2: 0.4968\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.21872 to 0.21257, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 13/30\n",
            "30/30 [==============================] - 7s 241ms/step - loss: 0.1709 - dice_coef: 0.3662 - iou: 0.2244 - recall_2: 0.1327 - precision_2: 0.9893 - val_loss: 0.2031 - val_dice_coef: 0.1232 - val_iou: 0.0658 - val_recall_2: 0.0042 - val_precision_2: 0.8348\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.21257 to 0.20312, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 14/30\n",
            "30/30 [==============================] - 7s 240ms/step - loss: 0.1664 - dice_coef: 0.3744 - iou: 0.2307 - recall_2: 0.1334 - precision_2: 0.9898 - val_loss: 0.1948 - val_dice_coef: 0.1506 - val_iou: 0.0817 - val_recall_2: 0.0153 - val_precision_2: 0.9452\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.20312 to 0.19480, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 15/30\n",
            "30/30 [==============================] - 7s 243ms/step - loss: 0.1619 - dice_coef: 0.3830 - iou: 0.2373 - recall_2: 0.1337 - precision_2: 0.9912 - val_loss: 0.1820 - val_dice_coef: 0.1922 - val_iou: 0.1068 - val_recall_2: 0.0333 - val_precision_2: 0.9940\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.19480 to 0.18205, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 16/30\n",
            "30/30 [==============================] - 7s 241ms/step - loss: 0.1580 - dice_coef: 0.3915 - iou: 0.2439 - recall_2: 0.1344 - precision_2: 0.9912 - val_loss: 0.1781 - val_dice_coef: 0.2041 - val_iou: 0.1142 - val_recall_2: 0.0390 - val_precision_2: 0.9935\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.18205 to 0.17814, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 17/30\n",
            "30/30 [==============================] - 7s 241ms/step - loss: 0.1547 - dice_coef: 0.3978 - iou: 0.2487 - recall_2: 0.1337 - precision_2: 0.9914 - val_loss: 0.1675 - val_dice_coef: 0.2445 - val_iou: 0.1401 - val_recall_2: 0.0588 - val_precision_2: 0.9966\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.17814 to 0.16748, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 18/30\n",
            "30/30 [==============================] - 7s 241ms/step - loss: 0.1513 - dice_coef: 0.4058 - iou: 0.2550 - recall_2: 0.1345 - precision_2: 0.9919 - val_loss: 0.1564 - val_dice_coef: 0.2830 - val_iou: 0.1657 - val_recall_2: 0.0717 - val_precision_2: 0.9990\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.16748 to 0.15641, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 19/30\n",
            "30/30 [==============================] - 8s 240ms/step - loss: 0.1475 - dice_coef: 0.4145 - iou: 0.2620 - recall_2: 0.1352 - precision_2: 0.9930 - val_loss: 0.1501 - val_dice_coef: 0.3125 - val_iou: 0.1862 - val_recall_2: 0.0867 - val_precision_2: 0.9987\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.15641 to 0.15009, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 20/30\n",
            "30/30 [==============================] - 7s 241ms/step - loss: 0.1444 - dice_coef: 0.4224 - iou: 0.2683 - recall_2: 0.1356 - precision_2: 0.9937 - val_loss: 0.1447 - val_dice_coef: 0.3343 - val_iou: 0.2016 - val_recall_2: 0.0954 - val_precision_2: 0.9986\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.15009 to 0.14468, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 21/30\n",
            "30/30 [==============================] - 7s 240ms/step - loss: 0.1420 - dice_coef: 0.4286 - iou: 0.2733 - recall_2: 0.1357 - precision_2: 0.9934 - val_loss: 0.1416 - val_dice_coef: 0.3518 - val_iou: 0.2144 - val_recall_2: 0.1025 - val_precision_2: 0.9980\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.14468 to 0.14160, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 22/30\n",
            "30/30 [==============================] - 7s 241ms/step - loss: 0.1396 - dice_coef: 0.4340 - iou: 0.2777 - recall_2: 0.1354 - precision_2: 0.9933 - val_loss: 0.1349 - val_dice_coef: 0.3942 - val_iou: 0.2463 - val_recall_2: 0.1272 - val_precision_2: 0.9966\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.14160 to 0.13489, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 23/30\n",
            "30/30 [==============================] - 7s 241ms/step - loss: 0.1381 - dice_coef: 0.4390 - iou: 0.2817 - recall_2: 0.1357 - precision_2: 0.9921 - val_loss: 0.1385 - val_dice_coef: 0.3744 - val_iou: 0.2313 - val_recall_2: 0.1207 - val_precision_2: 0.9961\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.13489\n",
            "Epoch 24/30\n",
            "30/30 [==============================] - 7s 242ms/step - loss: 0.1362 - dice_coef: 0.4429 - iou: 0.2850 - recall_2: 0.1349 - precision_2: 0.9922 - val_loss: 0.1377 - val_dice_coef: 0.3983 - val_iou: 0.2498 - val_recall_2: 0.1300 - val_precision_2: 0.9932\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.13489\n",
            "Epoch 25/30\n",
            "30/30 [==============================] - 7s 242ms/step - loss: 0.1329 - dice_coef: 0.4530 - iou: 0.2934 - recall_2: 0.1364 - precision_2: 0.9946 - val_loss: 0.1323 - val_dice_coef: 0.4063 - val_iou: 0.2557 - val_recall_2: 0.1358 - val_precision_2: 0.9956\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.13489 to 0.13232, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 26/30\n",
            "30/30 [==============================] - 8s 241ms/step - loss: 0.1304 - dice_coef: 0.4600 - iou: 0.2993 - recall_2: 0.1366 - precision_2: 0.9947 - val_loss: 0.1273 - val_dice_coef: 0.4204 - val_iou: 0.2669 - val_recall_2: 0.1345 - val_precision_2: 0.9957\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.13232 to 0.12726, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 27/30\n",
            "30/30 [==============================] - 7s 241ms/step - loss: 0.1284 - dice_coef: 0.4662 - iou: 0.3046 - recall_2: 0.1373 - precision_2: 0.9957 - val_loss: 0.1304 - val_dice_coef: 0.4133 - val_iou: 0.2615 - val_recall_2: 0.1375 - val_precision_2: 0.9841\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.12726\n",
            "Epoch 28/30\n",
            "30/30 [==============================] - 7s 241ms/step - loss: 0.1267 - dice_coef: 0.4712 - iou: 0.3089 - recall_2: 0.1370 - precision_2: 0.9959 - val_loss: 0.1255 - val_dice_coef: 0.4374 - val_iou: 0.2808 - val_recall_2: 0.1451 - val_precision_2: 0.9905\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.12726 to 0.12549, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 29/30\n",
            "30/30 [==============================] - 7s 241ms/step - loss: 0.1250 - dice_coef: 0.4769 - iou: 0.3138 - recall_2: 0.1376 - precision_2: 0.9961 - val_loss: 0.1242 - val_dice_coef: 0.4315 - val_iou: 0.2762 - val_recall_2: 0.1363 - val_precision_2: 0.9924\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.12549 to 0.12417, saving model to trained_models/model_hrf_combined.h5\n",
            "Epoch 30/30\n",
            "30/30 [==============================] - 7s 242ms/step - loss: 0.1237 - dice_coef: 0.4812 - iou: 0.3175 - recall_2: 0.1377 - precision_2: 0.9960 - val_loss: 0.1241 - val_dice_coef: 0.4346 - val_iou: 0.2784 - val_recall_2: 0.1371 - val_precision_2: 0.9945\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.12417 to 0.12407, saving model to trained_models/model_hrf_combined.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3eb2156250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf5sR7qWTChN"
      },
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import CustomObjectScope\n",
        "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
        "\n",
        "H = 512\n",
        "W = 768\n",
        "\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def read_image(path):\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    # x = cv2.resize(x, (W, H))\n",
        "    ori_x = x\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return ori_x, x\n",
        "\n",
        "def read_mask(path):\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)  ## (512, 512)\n",
        "    # x = cv2.resize(x, (W, H))\n",
        "    ori_x = x\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    x = x[:,:,0:2] # Discard the last channel \n",
        "    return ori_x, x\n",
        "\n",
        "def load_data(path):\n",
        "    x = sorted(glob(os.path.join(path, \"image\", \"*.jpg\")))\n",
        "    y = sorted(glob(os.path.join(path, \"mask\", \"*.jpg\")))\n",
        "    return x, y\n",
        "\n",
        "def save_results(ori_x, ori_y, y_pred, save_image_path, channel):\n",
        "    line = np.ones((y_pred.shape[0], 10, 3)) * 255\n",
        "\n",
        "    pred_image = np.zeros((y_pred.shape[0], y_pred.shape[1], 3))\n",
        "    _y_pred = y_pred[:, :, channel]\n",
        "    _ori_y = ori_y[:, :, channel]\n",
        "    pred_image[:, :, 0] = ((_y_pred > 0.5) & (_ori_y <= 128)) * 255\n",
        "    pred_image[:, :, 1] = ((_y_pred > 0.5) & (_ori_y  > 128)) * 255\n",
        "    pred_image[:, :, 2] = ((_ori_y  > 128) & (_y_pred <= 0.5 )) * 255\n",
        "\n",
        "    print(\" saving result\", save_image_path)\n",
        "    cv2.imwrite(save_image_path, pred_image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb1rxBB_WRci"
      },
      "source": [
        "def get_filenames_sorted(image_path, gt_path=None):\n",
        "    image_file_names = image_files(image_path)\n",
        "    gt_file_names = None\n",
        "    if gt_path is not None:\n",
        "        # Get the test files also\n",
        "        gt_file_names = sorted(image_files(gt_path))\n",
        "    return image_file_names, gt_file_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjoOD81UWZ96"
      },
      "source": [
        "def image_files(path, file_prefix=None):\n",
        "    all_image_files = []\n",
        "    if file_prefix is None:\n",
        "        file_names = list(glob(path + \"/*.*\" ))\n",
        "    else:\n",
        "        prefix___ = path + \"/\" + file_prefix + \".*\"\n",
        "        print(prefix___)\n",
        "        file_names = list(glob(prefix___))\n",
        "\n",
        "\n",
        "    for filename in file_names:\n",
        "        ext = filename.rsplit(\".\", 1)[1]\n",
        "        if ext in supported_extensions:\n",
        "            all_image_files.append(filename)\n",
        "    return sorted(all_image_files)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOfMiysOVyzk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee86ab73-67a4-4ee4-8d5d-5d6dd85fd042"
      },
      "source": [
        "data_dir = \"hrf_resized\"\n",
        "od_channel, bv_channel = 0, 1\n",
        "\n",
        "od_result_dir = result_dir + \"/od_predictions\"\n",
        "bv_result_dir = result_dir + \"/bv_predictions\"\n",
        "\n",
        "create_dir(od_result_dir)\n",
        "create_dir(bv_result_dir)\n",
        "\n",
        "\"\"\" Load the model \"\"\"\n",
        "model_file_name = f\"{model_dir}/{model_name}.h5\"\n",
        "print(model_file_name)\n",
        "with CustomObjectScope({'iou': iou, 'dice_coef': dice_coef, 'dice_loss': dice_loss}):\n",
        "    model = tf.keras.models.load_model(model_file_name)\n",
        "\n",
        "\"\"\" Load the dataset \"\"\"\n",
        "# dataset_path = os.path.join(data_dir, \"test\")\n",
        "supported_extensions = [\"png\", \"jpg\", \"tif\", \"jpeg\", \"gif\"]\n",
        "\n",
        "test_x, test_y = get_filenames_sorted(data_dir + \"/test/image/\", data_dir + \"/test/mask/\" )\n",
        "\n",
        "\"\"\" Make the prediction and calculate the metrics values \"\"\"\n",
        "SCORE_BV, SCORE_OD = [], []\n",
        "for x, y in tqdm(zip(test_x, test_y), total=len(test_x)):\n",
        "    \"\"\" Extracting name \"\"\"\n",
        "    name = x.rsplit(\"/\", 1)[1].rsplit(\".\", 1)[0]\n",
        "    print(name)\n",
        "\n",
        "    \"\"\" Read the image and mask \"\"\"\n",
        "    ori_x, x = read_image(x)\n",
        "    ori_y, y = read_mask(y)\n",
        "\n",
        "    \"\"\" Prediction \"\"\"\n",
        "    y_pred = model.predict(np.expand_dims(x, axis=0))[0]\n",
        "    y_pred = y_pred > 0.5\n",
        "    y_pred = y_pred.astype(np.float32)\n",
        "\n",
        "    print(np.max(ori_y), np.max(y))\n",
        "\n",
        "    \"\"\" Saving the images \"\"\"\n",
        "    save_image_path_od = f\"{od_result_dir}/{name}.png\"\n",
        "    save_results(ori_x, ori_y, y_pred, save_image_path_od, od_channel)\n",
        "\n",
        "    save_image_path_bv = f\"{bv_result_dir}/{name}.png\"\n",
        "    save_results(ori_x, ori_y, y_pred, save_image_path_bv, bv_channel)\n",
        "\n",
        "\n",
        "#     \"\"\" Calculate the bv metrics \"\"\"\n",
        "    bv_pred = y_pred[:, :, bv_channel].flatten()\n",
        "    bv_gt = y[:, :, bv_channel].flatten()\n",
        "    acc_value = accuracy_score(bv_gt > 0.5, bv_pred>0.5)\n",
        "    f1_value = f1_score(bv_gt > 0.5, bv_pred>0.5, labels=[0, 1], average=\"binary\")\n",
        "    jac_value = jaccard_score(bv_gt > 0.5, bv_pred>0.5, labels=[0, 1], average=\"binary\")\n",
        "    recall_value = recall_score(bv_gt > 0.5, bv_pred>0.5, labels=[0, 1], average=\"binary\")\n",
        "    recall_computed = np.sum((bv_gt > 0.5) & (bv_pred > 0.5)) / np.sum(bv_gt > 0.5)\n",
        "    precision_value = precision_score(bv_gt > 0.5, bv_pred>0.5, labels=[0, 1], average=\"binary\")\n",
        "    SCORE_BV.append([name, acc_value, f1_value, jac_value, recall_value, precision_value])\n",
        "\n",
        "#     \"\"\" Calculate the od metrics \"\"\"\n",
        "    bv_pred = y_pred[:, :, od_channel].flatten()\n",
        "    bv_gt = y[:, :, od_channel].flatten()\n",
        "    acc_value = accuracy_score(bv_gt > 0.5, bv_pred>0.5)\n",
        "    f1_value = f1_score(bv_gt > 0.5, bv_pred>0.5, labels=[0, 1], average=\"binary\")\n",
        "    jac_value = jaccard_score(bv_gt > 0.5, bv_pred>0.5, labels=[0, 1], average=\"binary\")\n",
        "    recall_value = recall_score(bv_gt > 0.5, bv_pred>0.5, labels=[0, 1], average=\"binary\")\n",
        "    recall_computed = np.sum((bv_gt > 0.5) & (bv_pred > 0.5)) / np.sum(bv_gt > 0.5)\n",
        "    precision_value = precision_score(bv_gt > 0.5, bv_pred>0.5, labels=[0, 1], average=\"binary\")\n",
        "    SCORE_OD.append([name, acc_value, f1_value, jac_value, recall_value, precision_value])\n",
        "    \n",
        "print(\"\\n\")\n",
        "for SCORE in [SCORE_OD, SCORE_BV]:\n",
        "    if SCORE == SCORE_OD:\n",
        "        print(\"****** OD Metrics\")\n",
        "    else:\n",
        "        print(\"****** BV Metrics\")\n",
        "    score = [s[1:] for s in SCORE]\n",
        "    score = np.mean(score, axis=0)\n",
        "    print(f\"Accuracy: {score[0]:0.5f}\")\n",
        "    print(f\"F1: {score[1]:0.5f} (dice score)\")\n",
        "    print(f\"AUC: {score[1]:0.5f} (Auc score)\")\n",
        "    print(f\"Jaccard: {score[2]:0.5f}\")\n",
        "    print(f\"Recall: {score[3]:0.5f}\")\n",
        "    print(f\"Precision: {score[4]:0.5f}\")\n",
        "\n",
        "    # \"\"\" Saving \"\"\"\n",
        "    if SCORE == SCORE_OD:\n",
        "        df = pd.DataFrame(SCORE, columns=[\"Image\", \"Acc\", \"F1\", \"Jaccard\", \"Recall\", \"Precision\"])\n",
        "        df.to_csv(f\"{od_result_dir}/score.csv\")\n",
        "    else:\n",
        "        df = pd.DataFrame(SCORE, columns=[\"Image\", \"Acc\", \"F1\", \"Jaccard\", \"Recall\", \"Precision\"])\n",
        "        df.to_csv(f\"{bv_result_dir}/score.csv\")\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trained_models/model_hrf_combined.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11_h\n",
            "255 1.0\n",
            " saving result hrf_combined_results/od_predictions/11_h.png\n",
            " saving result hrf_combined_results/bv_predictions/11_h.png\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [00:01<00:05,  1.31s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "12_h\n",
            "255 1.0\n",
            " saving result hrf_combined_results/od_predictions/12_h.png\n",
            " saving result hrf_combined_results/bv_predictions/12_h.png\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 2/5 [00:02<00:03,  1.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13_h\n",
            "255 1.0\n",
            " saving result hrf_combined_results/od_predictions/13_h.png\n",
            " saving result hrf_combined_results/bv_predictions/13_h.png\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 3/5 [00:03<00:02,  1.16s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "14_h\n",
            "255 1.0\n",
            " saving result hrf_combined_results/od_predictions/14_h.png\n",
            " saving result hrf_combined_results/bv_predictions/14_h.png\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 4/5 [00:04<00:01,  1.12s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "15_h\n",
            "255 1.0\n",
            " saving result hrf_combined_results/od_predictions/15_h.png\n",
            " saving result hrf_combined_results/bv_predictions/15_h.png\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:05<00:00,  1.08s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "****** OD Metrics\n",
            "Accuracy: 0.99494\n",
            "F1: 0.72737 (dice score)\n",
            "AUC: 0.72737 (Auc score)\n",
            "Jaccard: 0.57804\n",
            "Recall: 0.58043\n",
            "Precision: 0.99347\n",
            "\n",
            "\n",
            "****** BV Metrics\n",
            "Accuracy: 0.96322\n",
            "F1: 0.78299 (dice score)\n",
            "AUC: 0.78299 (Auc score)\n",
            "Jaccard: 0.64572\n",
            "Recall: 0.76218\n",
            "Precision: 0.81338\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoIjM_T7WBWY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}